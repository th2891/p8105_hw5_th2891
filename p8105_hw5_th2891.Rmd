---
title: "Iteration and listcols"
output: github_document
---

```{r setup}
library(tidyverse)
library(rvest)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6, 
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "virids"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

Downloading & cleaning data, creating a city_state variable, create `resolution` variable with outcomes; "solved" or "unsolved." Filtering out data entry error "TulsaAL."

```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state), 
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "TulsaAL")
```


Focus on Baltimore

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "BaltimoreMD")

baltimore_summary = 
baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test = 
  prop.test(
  x = baltimore_summary %>%  pull(unsolved),
  n = baltimore_summary %>%  pull(n))

baltimore_test %>% 
  broom::tidy()
```

Iterate across cities

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

Create & test function

```{r}
prop_test_function = function(city_df){
  
  city_summary = 
    city_df %>% 
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
  )

  city_test = 
    prop.test(
      x = city_summary %>%  pull(unsolved),
      n = city_summary %>%  pull(n))
  
  return(city_test)
  
}
homicide_df %>% 
  filter(city_state == "AlbuquerqueNM") %>% 
prop_test_function()
```

Iterating across all cities
```{r}
results_df = 
  homicide_df %>%
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 2
write function to iterate across 'file' and use read_csv to import each file
- end up with column of data sets where first column is first data set
- then unnest that column and have weeks
- tidy that result 
    - separate file names to get control experiment, unique subject id, want long format (in wide format)
- want organized data set where you can make spaghetti plot for each participant to see if there are differences between groups


read_csv iteration 

```{r}

list_files = list.files("./data/zip_data/", pattern = ".csv")

csv_file_names = 
  list_files %>% 
  set_names() %>% 
  map_dfr(~read_csv(paste0("./data/zip_data/",.x), col_types = cols()),
          .id = "tx_id"
      ) %>% 
  mutate(
    tx_id = str_remove(tx_id, "\\.[^.]*$")) 
```

Make plot
* need to group by tx prefix 

```{r}
csv_file_names %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "results"
  ) %>% 
  ggplot(aes(x = week, y = results, group = tx_id, color = tx_id)) +
  geom_path() + 
  labs(caption = "Weekly results by treatment group (control vs. experimant)")

```




## Problem 3

```{r}
library(tidyverse)

set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Numeric - Replace "n/a" with mean of column 
```{r}
iris_with_missing %>% 
  mutate(Sepal.Length = 
           replace(Sepal.Length, is.na(Sepal.Length), mean(Sepal.Length, na.rm = TRUE)))

```

Character- Replace "na" with "virginica"

```{r}
iris_with_missing %>% 
  mutate(Species = 
           replace(Species, is.na(Species), "virginica"))
```


write a function.
```{r}
fill_in_missing = function(vector){
  if (is.numeric  ){
    mutate(.x = 
             replace(.x, is.na(.x), mean(.x, na.rm = TRUE)))
  
  }}

```

check function pulling out columns 

map accross each column and apply that same function

